apiVersion: ai.sap.com/v1alpha1
kind: ServingTemplate
metadata:
  name: sarimax-serve-deployment
  annotations:
    scenarios.ai.sap.com/description: "SARIMAX model serving with KServe"
    scenarios.ai.sap.com/name: "sarimax-timeseries"
    executables.ai.sap.com/description: "Deploy SARIMAX forecasting models for persistent serving"
    executables.ai.sap.com/name: "serve-deployment"
  labels:
    scenarios.ai.sap.com/id: "sarimax-timeseries"
    executables.ai.sap.com/id: "serve-deployment"
    ai.sap.com/version: "1.0.0"
spec:
  inputs:
    parameters:
      - name: imageName
        type: string
        default: "docker.io/priyaannamalai/sarimax-forecasting:v1"
      - name: modelName
        type: string
        default: "sarimax_initial_18months"
  template:
    apiVersion: "serving.kserve.io/v1beta1"
    kind: "InferenceService"
    metadata:
      annotations: |
        autoscaling.knative.dev/metric: concurrency
        autoscaling.knative.dev/target: "1"
        autoscaling.knative.dev/targetBurstCapacity: "0"
      labels: |
        ai.sap.com/resourcePlan: infer.s
    spec: |
      predictor:
        minReplicas: 1
        maxReplicas: 3
        containers:
          - name: kserve-container
            image: "{{inputs.parameters.imageName}}"
            ports:
              - containerPort: 9001
                protocol: TCP
            env:
              - name: MODEL_NAME
                value: "{{inputs.parameters.modelName}}"
              - name: FLASK_ENV
                value: "production"
            resources:
              limits:
                cpu: "2"
                memory: "4Gi"
              requests:
                cpu: "1"
                memory: "2Gi"
